
\section{Conclusion and comments}

This report illustrates the comparisons between each type of models predicting heart disease based solely on specific demographical and clinical measures. In general, a \( k \)-nearest neighbour model, where \( k = 19 \), and logistic regression model outperform decision trees, with an AUC-score of around \( 0.93 \) compared to \( 0.88 \). Precision-recall-wise, logistic model shows performance stable lead around the \( \delta = 0.75 \) threshold, whilst \( 19 \)-NN lags behind.

For real-life application, \textbf{this report would support a logistic regression model}, thanks to its interpretibility and goodness-of-fit compared to other models. Indeed, this model illustrates solid performance in all metrics, with the highest AUC-score and a stable precision-recall curve. Also, by looking at the coefficients of the model, one can deduce the importance of each feature in contributing to the disease status.

This is not present in other experiments. Decision trees, despite its simplicity, fail to give reliable results and a detailed explaination, as factors like \texttt{heart.rate} or \texttt{angina} are never considered. In contrast, although the best nearest neighbour model perform equivalently to a logistic model, there is no simple explaination on how the model would interpret the features other than a simple distance-based rule. Another drawback of \(k\)-NN is \textit{the curse of dimensionality}, as the model is essentially working on a \( \mathbb R^9 \) numerical space in which the difference in distances are negligible. Had the dataset got more features, this effect would have been a significant problem.

For further analysis, this report would suggest that some kind of dimensionality reduction is applied on the data (for instance, Principal Component Analysis, \( \mathbb R^9 \mapsto \mathbb R^2 \)) so that nearest neighbour models can work on a less-dimensional, compact space. Other models such as Na\"ive Bayes can be considered for similar small datasets. Finally, one of the big challenges in this report is the size of the dataset, with only \( n = 298 \) records after cleaning. One could try evaluting the models on a \( 918 \)-record dataset from the UCI Machine Learning Repository \citep{extradataset} and would probably produce more reliable results.